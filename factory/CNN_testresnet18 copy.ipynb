{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import matplotlib.pyplot as plt\n",
    "import modules.cosmos_functions as cf\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to use in the tests \n",
    "\n",
    "# functions to display images\n",
    "def reverse_normalize(image):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    image = image.clone()\n",
    "    for i in range(3):\n",
    "        image[i] = (image[i] * std[i]) + mean[i]\n",
    "    return image\n",
    "\n",
    "def show_batch(test_d):\n",
    "    # Get the first batch of data from the DataLoader\n",
    "    data_test = next(iter(test_d))\n",
    "\n",
    "    # Retrieve the first tensor and its corresponding label\n",
    "    image_test = data_test[0][0]\n",
    "    label_test = data_test[1][0]\n",
    "\n",
    "    # Reverse the normalization of the image\n",
    "    image_test = reverse_normalize(image_test)\n",
    "\n",
    "    # Convert the image tensor to a NumPy array and transpose the dimensions\n",
    "    np_image_test = image_test.permute(1, 2, 0).numpy()\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(np_image_test)\n",
    "    plt.title(f'{label_test}, {image_test.shape}')\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "# function to test the model\n",
    "def test_model(model, datasetPath):\n",
    "    model.eval()    \n",
    "\n",
    "    # Load the test dataset\n",
    "    dataset_path = datasetPath\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor()\n",
    "                \n",
    "])\n",
    "    dataset = ImageFolder(dataset_path, transform=transform)\n",
    "    test_dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    labels_dict = dataset.class_to_idx\n",
    "\n",
    "    # Track the overall test accuracy and accuracy by each type of apple\n",
    "    overall_correct = 0\n",
    "    overall_total = 0\n",
    "    normal_correct = 0\n",
    "    normal_total = 0\n",
    "    abnormal_correct = 0\n",
    "    abnormal_total = 0\n",
    "\n",
    "    # Initialize the confusion matrix\n",
    "    num_classes = len(labels_dict)\n",
    "    confusion_matrix = np.zeros((num_classes, num_classes), dtype=int)\n",
    "\n",
    "    # Iterate over the test dataset\n",
    "    for images, labels in test_dataloader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Get predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        # Update accuracy counts\n",
    "        overall_correct += (predicted == labels).sum().item()\n",
    "        overall_total += labels.size(0)\n",
    "\n",
    "        # Calculate accuracy for normal apples vs. abnormal apples\n",
    "        normal_mask = labels == labels_dict['Normal_Apple']\n",
    "        abnormal_mask = ~normal_mask\n",
    "        normal_correct += (predicted[normal_mask] == labels[normal_mask]).sum().item()\n",
    "        normal_total += normal_mask.sum().item()\n",
    "        abnormal_correct += (predicted[abnormal_mask] == labels[abnormal_mask]).sum().item()\n",
    "        abnormal_total += abnormal_mask.sum().item()\n",
    "\n",
    "        # Update the confusion matrix\n",
    "        for true_label, predicted_label in zip(labels.cpu().numpy(), predicted.cpu().numpy()):\n",
    "            confusion_matrix[true_label][predicted_label] += 1\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = overall_correct / overall_total\n",
    "\n",
    "    # Calculate accuracy for normal apples and abnormal apples separately\n",
    "    normal_accuracy = normal_correct / normal_total if normal_total != 0 else 0.0\n",
    "    abnormal_accuracy = abnormal_correct / abnormal_total if abnormal_total != 0 else 0.0\n",
    "\n",
    "    # Print overall accuracy\n",
    "    print(f\"Overall accuracy: {overall_accuracy:.4f}\")\n",
    "\n",
    "    # Print accuracy for normal apples and abnormal apples separately\n",
    "    print(f\"Normal Apple accuracy: {normal_accuracy:.4f}\")\n",
    "    print(f\"Abnormal Apple accuracy: {abnormal_accuracy:.4f}\")\n",
    "\n",
    "    # Print the confusion matrix\n",
    "    print()\n",
    "    print(labels_dict)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is 'mps' Thus a tensor will look like this: tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# set the device\n",
    "\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "x = torch.ones(1, device=device)\n",
    "\n",
    "print(f\"Device is '{device}' Thus a tensor will look like this: {x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the resnet18 model for testing the test dataset with the trained model\n",
    "# import the resnet18 model\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT  #weights=ResNet18_Weights.IMAGENET1K_V1 is the current default\n",
    "model = resnet18(weights=weights)  \n",
    "\n",
    "# freeze the model parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "# change the last layer of the model to fit the number of classes in the dataset\n",
    "model.fc = nn.Linear(512, 4)\n",
    "    \n",
    "# change the last layer of the model to fit the number of classes in the dataset\n",
    "model.fc = nn.Linear(512, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the 1st img in the dataset\n",
    "# show_batch(test_d)\n",
    "# print(test_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imported_model_path = \"../storage/data/generated/20230605-134750_pinky_acc.pt\"  # high accuracy\n",
    "# imported_model_path = cf.load_pth('20230605_160852_pinky')  # issues; WIP\n",
    "# imported_model_path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6917\n",
      "Normal Apple accuracy: 0.2500\n",
      "Abnormal Apple accuracy: 0.8021\n",
      "\n",
      "{'Blotch_Apple': 0, 'Normal_Apple': 1, 'Rot_Apple': 2, 'Scab_Apple': 3}\n",
      "Confusion Matrix:\n",
      "[[25  0  0  5]\n",
      " [11  6  1  6]\n",
      " [ 0  0 36  2]\n",
      " [ 5  1  6 16]]\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = \"../storage/images/apple_resized_128/Test\"\n",
    "\n",
    "# import the model state\n",
    "imported_model_state_path = \"../storage/data/generated/20230608-233306_pinky_acc.pt\"   # test to 128x128 accuracy\n",
    "\n",
    "# load the model state into the model\n",
    "model_state_import_path = imported_model_state_path\n",
    "model.load_state_dict(torch.load(model_state_import_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_model(model, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7583\n",
      "Normal Apple accuracy: 0.5000\n",
      "Abnormal Apple accuracy: 0.8229\n",
      "\n",
      "{'Blotch_Apple': 0, 'Normal_Apple': 1, 'Rot_Apple': 2, 'Scab_Apple': 3}\n",
      "Confusion Matrix:\n",
      "[[27  0  1  2]\n",
      " [ 6 12  1  5]\n",
      " [ 0  1 36  1]\n",
      " [ 4  5  3 16]]\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = \"../storage/images/apple_resized_128/Test\"\n",
    "\n",
    "# import the model state\n",
    "imported_model_state_path = \"../storage/data/generated/20230608-233306_pinky_loss.pt\"   # test to 128x128 loss\n",
    "\n",
    "# load the model state into the model\n",
    "model_state_import_path = imported_model_state_path\n",
    "model.load_state_dict(torch.load(model_state_import_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_model(model, dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.6917\n",
      "Normal Apple accuracy: 0.2500\n",
      "Abnormal Apple accuracy: 0.8021\n",
      "\n",
      "{'Blotch_Apple': 0, 'Normal_Apple': 1, 'Rot_Apple': 2, 'Scab_Apple': 3}\n",
      "Confusion Matrix:\n",
      "[[25  0  0  5]\n",
      " [11  6  1  6]\n",
      " [ 0  0 36  2]\n",
      " [ 5  1  6 16]]\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = \"../storage/apple_disease_classification_unedited/Test\"\n",
    "\n",
    "# import the model state\n",
    "imported_model_state_path = \"../storage/data/generated/20230608-233306_pinky_acc.pt\"   # test to 128x128 accuracy\n",
    "\n",
    "# load the model state into the model\n",
    "model_state_import_path = imported_model_state_path\n",
    "model.load_state_dict(torch.load(model_state_import_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_model(model, dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall accuracy: 0.7583\n",
      "Normal Apple accuracy: 0.5000\n",
      "Abnormal Apple accuracy: 0.8229\n",
      "\n",
      "{'Blotch_Apple': 0, 'Normal_Apple': 1, 'Rot_Apple': 2, 'Scab_Apple': 3}\n",
      "Confusion Matrix:\n",
      "[[27  0  1  2]\n",
      " [ 6 12  1  5]\n",
      " [ 0  1 36  1]\n",
      " [ 4  5  3 16]]\n"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = \"../storage/images/apple_disease_classification_unedited/Test\"\n",
    "\n",
    "# import the model state\n",
    "imported_model_state_path = \"../storage/data/generated/20230608-233306_pinky_loss.pt\"   # test to 128x128 loss\n",
    "\n",
    "# load the model state into the model\n",
    "model_state_import_path = imported_model_state_path\n",
    "model.load_state_dict(torch.load(model_state_import_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_model(model, dataset_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 138, 123] at entry 0 and [3, 336, 335] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     10\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mFinal model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m test_model(model, dataset_path)\n",
      "Cell \u001b[0;32mIn[3], line 62\u001b[0m, in \u001b[0;36mtest_model\u001b[0;34m(model, datasetPath)\u001b[0m\n\u001b[1;32m     59\u001b[0m confusion_matrix \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((num_classes, num_classes), dtype\u001b[39m=\u001b[39m\u001b[39mint\u001b[39m)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Iterate over the test dataset\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m test_dataloader:\n\u001b[1;32m     63\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     64\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;49;00m samples \u001b[39min\u001b[39;49;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[1;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/pink_lady-bMzUwwsu/lib/python3.11/site-packages/torch/utils/data/_utils/collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 138, 123] at entry 0 and [3, 336, 335] at entry 1"
     ]
    }
   ],
   "source": [
    "# Load the test dataset\n",
    "dataset_path = \"../storage/images/apple_disease_classification_unedited/Test\"\n",
    "\n",
    "# import the model state\n",
    "imported_model_state_path = \"../storage/data/generated/20230608-233306_pinky_final.pt\"   # test to 128x128 loss\n",
    "\n",
    "# load the model state into the model\n",
    "model_state_import_path = imported_model_state_path\n",
    "model.load_state_dict(torch.load(model_state_import_path))\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Final model\")\n",
    "test_model(model, dataset_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pink_lady-bMzUwwsu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
