{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FV8u8dT4C17"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pipenv install -U sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "model = SentenceTransformer('all-MiniLM-L12-v2')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
        "\n",
        "# get the query from the user\n",
        "query_input = input('What is your question?')\n",
        "\n",
        "# encode the query using the model\n",
        "query_embedding = model.encode(query_input)\n",
        "passage_embedding = model.encode(['How did the batch turn out?',\n",
        "                                  'was the batch accepted?',\n",
        "                                  'Was the batch rejected',\n",
        "                                  'what quality class is this batch?',\n",
        "                                  'The quality of this batch is very bad, why?',\n",
        "                                  'Why was this batch rejected?',\n",
        "                                  'How many apples were rejected?',\n",
        "                                  'How many apples were tested?',\n",
        "                                  'How many apples were good?',\n",
        "                                  'How many apples were bad?'])\n",
        "\n",
        "# get the similarity scores of first query with all other queries\n",
        "similarity_scores = util.dot_score(query_embedding, passage_embedding)\n",
        "softmax_scores = torch.softmax(similarity_scores, dim=1)\n",
        "\n",
        "# Get the label of the most appropriate result\n",
        "max_score, max_index = torch.max(softmax_scores, dim=1)\n",
        "label = max_index.item()\n",
        "\n",
        "# # labeling test prints\n",
        "# print(\"Similarity:\", similarity_scores)\n",
        "# print(\"Softmax scores:\", softmax_scores)\n",
        "print(f'The most apropriate label for the question: {query_input} is: {label}')\n",
        "\n",
        "\n",
        "import random\n",
        "random = random.randint(0, 3)\n",
        "AQL_label = [random,1,22,0,9, 500, 32]\n",
        "\n",
        "\n",
        "def query_answer(label, test_results):\n",
        "    test_Klasse = test_results[0]\n",
        "    test_apple_rot = test_results[1]\n",
        "    test_appel_normal = test_results[2]\n",
        "    test_apple_blotch = test_results[3]\n",
        "    test_apple_spot = test_results[4]\n",
        "    test_lot_size = test_results[5]\n",
        "    test_batch_size = test_results[6]\n",
        "    \n",
        "    class_roman = {0: 'I', 1: 'II', 2: 'III'}\n",
        "\n",
        "    def reject_norm(test_Klasse):\n",
        "        if test_Klasse >= 2:\n",
        "            return 'The batch was accepted'\n",
        "        return 'The batch was rejected'\n",
        "\n",
        "    if label == 0:\n",
        "        return f'{reject_norm(test_Klasse)} with Class_{class_roman[test_Klasse]}'\n",
        "    elif label == 1 or label == 2:\n",
        "        return reject_norm(test_Klasse)\n",
        "    elif label == 3:\n",
        "        return f'The batch was assigned Class_{class_roman[test_Klasse]}'\n",
        "    elif label == 4:\n",
        "        return f\"Too many apples {test_apple_rot+test_apple_blotch+test_apple_spot} were rejected, batch is deemed unfit to be processed in human foostuffs\"\n",
        "    elif label == 5:\n",
        "        return f'Of the batch of {test_lot_size} apples, {test_batch_size} were tested. {test_apple_rot} apples were rejected because of rot, {test_apple_blotch} apples were rejected because of blotch, {test_apple_spot} apples were rejected because of spot'\n",
        "    elif label == 6 or label == 9:\n",
        "        return f' {test_batch_size} apples were tested {test_batch_size-test_appel_normal} apples were rejected because of rot, blotch or spot, {test_appel_normal} apples were good'\n",
        "    elif label == 7:\n",
        "        return f' {test_batch_size} apples were tested of the lot of {test_lot_size} apples'\n",
        "    elif label == 8:\n",
        "        return f'Of the lot of {test_batch_size}, {test_appel_normal} apples were good'\n",
        "    else:\n",
        "        return 'I am sorry, I do not understand your question'\n",
        "    \n",
        "print(query_input)\n",
        "print(query_answer(label, AQL_label))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
